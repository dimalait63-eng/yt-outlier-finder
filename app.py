# app.py ‚Äî YouTube "Most Popular" Outlier Finder (–±–µ–∑ –∫–ª—é—á–µ–π, –≤—ã–±–æ—Ä —Å—Ç—Ä–∞–Ω—ã, –±–µ–∑ Shorts)
#
# –ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
# - –ë–µ—Ä—ë—Ç "—Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ" –≤–∏–¥–µ–æ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–µ (YouTube chart=mostPopular)
# - (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
# - –£–±–∏—Ä–∞–µ—Ç Shorts (–ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
# - –¢—è–Ω–µ—Ç –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤ –∫–∞–Ω–∞–ª–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ —Ç–≤–æ–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (subs / views / ratio / views_per_day)
#
# –ó–∞–ø—É—Å–∫ (–ª–æ–∫–∞–ª—å–Ω–æ –∏–ª–∏ –Ω–∞ Streamlit Cloud):
#   pip install streamlit google-api-python-client python-dotenv pandas
#   streamlit run app.py
#
# .env (–≤ –ø–∞–ø–∫–µ –ø—Ä–æ–µ–∫—Ç–∞):
#   YOUTUBE_API_KEY=–≤–∞—à_–∫–ª—é—á

import os
import re
from datetime import datetime, timezone
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY", "").strip()

st.set_page_config(page_title="YouTube Most Popular Outliers", layout="wide")
st.title("YouTube Most Popular Outliers (–±–µ–∑ –∫–ª—é—á–µ–π, –≤—ã–±–æ—Ä —Å—Ç—Ä–∞–Ω—ã, –±–µ–∑ Shorts)")

if not API_KEY:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω YOUTUBE_API_KEY. –°–æ–∑–¥–∞–π —Ñ–∞–π–ª .env –∏ –¥–æ–±–∞–≤—å –∫–ª—é—á.")
    st.stop()

youtube = build("youtube", "v3", developerKey=API_KEY)

DUR_RE = re.compile(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?")

def iso_duration_to_seconds(d: str) -> int:
    m = DUR_RE.fullmatch(d or "")
    if not m:
        return 0
    h = int(m.group(1) or 0)
    mi = int(m.group(2) or 0)
    s = int(m.group(3) or 0)
    return h * 3600 + mi * 60 + s

def age_days(published_at: str) -> float:
    # "2026-02-25T12:34:56Z"
    dt = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
    days = (datetime.now(timezone.utc) - dt).total_seconds() / 86400
    return max(days, 0.1)

def fmt_int(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return "‚Äî"
    try:
        return f"{int(x):,}".replace(",", " ")
    except Exception:
        return "‚Äî"

@st.cache_data(ttl=3600)
def fetch_categories(region_code: str):
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (id, title)
    r = youtube.videoCategories().list(part="snippet", regionCode=region_code).execute()
    cats = []
    for it in r.get("items", []):
        sn = it.get("snippet", {})
        if sn.get("assignable") is True:
            cats.append((it["id"], sn.get("title", it["id"])))
    # –Ω–∞ –≤—Å—è–∫–∏–π
    cats.sort(key=lambda x: x[1].lower())
    return cats

@st.cache_data(ttl=1800)
def most_popular_video_ids(region_code: str, pages: int, per_page: int, category_id: str | None):
    ids = []
    page_token = None
    for _ in range(pages):
        params = dict(
            part="id",
            chart="mostPopular",
            regionCode=region_code,
            maxResults=per_page,
        )
        if category_id:
            params["videoCategoryId"] = category_id
        if page_token:
            params["pageToken"] = page_token

        r = youtube.videos().list(**params).execute()
        ids.extend([it["id"] for it in r.get("items", [])])
        page_token = r.get("nextPageToken")
        if not page_token:
            break
    # dedupe, preserve order
    return list(dict.fromkeys(ids))

@st.cache_data(ttl=1800)
def fetch_videos(video_ids: list[str]):
    out = []
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        r = youtube.videos().list(
            part="snippet,statistics,contentDetails",
            id=",".join(chunk)
        ).execute()

        for it in r.get("items", []):
            sn = it.get("snippet", {})
            stt = it.get("statistics", {})
            cd = it.get("contentDetails", {})

            thumbs = sn.get("thumbnails", {})
            thumb_url = None
            for k in ["maxres", "standard", "high", "medium", "default"]:
                if k in thumbs and "url" in thumbs[k]:
                    thumb_url = thumbs[k]["url"]
                    break

            out.append({
                "videoId": it.get("id"),
                "title": sn.get("title", ""),
                "channelId": sn.get("channelId", ""),
                "channelTitle": sn.get("channelTitle", ""),
                "publishedAt": sn.get("publishedAt", ""),
                "views": int(stt.get("viewCount", 0)),
                "likes": int(stt.get("likeCount", 0)) if "likeCount" in stt else None,
                "comments": int(stt.get("commentCount", 0)) if "commentCount" in stt else None,
                "duration_iso": cd.get("duration", ""),
                "thumbnail": thumb_url,
            })
    return out

@st.cache_data(ttl=1800)
def fetch_channels_subs(channel_ids: list[str]):
    result = {}
    channel_ids = [c for c in channel_ids if c]
    for i in range(0, len(channel_ids), 50):
        chunk = channel_ids[i:i+50]
        r = youtube.channels().list(part="statistics,snippet", id=",".join(chunk)).execute()
        for it in r.get("items", []):
            stt = it.get("statistics", {})
            subs = stt.get("subscriberCount")
            result[it["id"]] = int(subs) if subs is not None else None
    return result

# -------- UI --------
COMMON_COUNTRIES = [
    ("US", "United States"),
    ("GB", "United Kingdom"),
    ("CA", "Canada"),
    ("AU", "Australia"),
    ("DE", "Germany"),
    ("FR", "France"),
    ("NL", "Netherlands"),
    ("ES", "Spain"),
    ("IT", "Italy"),
    ("BR", "Brazil"),
    ("MX", "Mexico"),
    ("JP", "Japan"),
    ("KR", "South Korea"),
    ("IN", "India"),
    ("RU", "Russia"),
    ("UA", "Ukraine"),
    ("PL", "Poland"),
    ("SE", "Sweden"),
    ("NO", "Norway"),
    ("TR", "Turkey"),
]

with st.sidebar:
    st.header("–ò—Å—Ç–æ—á–Ω–∏–∫")
    country_choice = st.selectbox(
        "–°—Ç—Ä–∞–Ω–∞ (regionCode)",
        options=COMMON_COUNTRIES,
        index=0,
        format_func=lambda x: f"{x[1]} ({x[0]})"
    )
    region_code = country_choice[0]

    st.caption("–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: YouTube chart=mostPopular. –≠—Ç–æ –Ω–µ –≤–µ—Å—å YouTube, –∞ —Ç—Ä–µ–Ω–¥–æ–≤—ã–π/–ø–æ–ø—É–ª—è—Ä–Ω—ã–π —Å—Ä–µ–∑ –ø–æ —Å—Ç—Ä–∞–Ω–µ.")

    st.subheader("–û–±—ä—ë–º –≤—ã–±–æ—Ä–∫–∏")
    pages = st.slider("–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±—Ä–∞—Ç—å", 1, 10, 3, 1)
    per_page = st.selectbox("–í–∏–¥–µ–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É", [10, 25, 50], index=2)
    use_categories = st.checkbox("–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—à–∏—Ä–µ –æ—Ö–≤–∞—Ç)", value=True)

    st.subheader("–§–∏–ª—å—Ç—Ä—ã")
    exclude_shorts = st.checkbox("–ò—Å–∫–ª—é—á–∏—Ç—å Shorts/–∫–æ—Ä–æ—Ç–∫–∏–µ", value=True)
    min_seconds = st.slider("–ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (—Å–µ–∫)", 60, 1800, 120, 30, disabled=not exclude_shorts)

    min_views = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", min_value=0, value=50_000, step=10_000)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–∏ –∫–∞–Ω–∞–ª–∞", min_value=0, value=200_000, step=10_000)
    min_ratio = st.number_input("–ú–∏–Ω. Views/Subs (–µ—Å–ª–∏ subs –≤–∏–¥–Ω—ã)", min_value=0.0, value=3.0, step=1.0)

    st.subheader("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞/–≤–∏–¥")
    sort_mode = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ", ["views_per_day", "ratio", "views", "date"], index=0)
    view_mode = st.radio("–í–∏–¥", ["–ö–∞—Ä—Ç–æ—á–∫–∏ (—Å –æ–±–ª–æ–∂–∫–∞–º–∏)", "–¢–∞–±–ª–∏—Ü–∞"], index=0)

    run = st.button("üîé –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å")

if not run:
    st.write("–í—ã–±–µ—Ä–∏ —Å—Ç—Ä–∞–Ω—É —Å–ª–µ–≤–∞ ‚Üí –Ω–∞–∂–º–∏ **–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å**.")
    st.stop()

errors = []
try:
    video_ids = []

    if use_categories:
        cats = fetch_categories(region_code)
        # –í–∞–∂–Ω–æ: –µ—Å–ª–∏ –≤—ã–±—Ä–∞—Ç—å –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü ‚Äî —Å—Ç–∞–Ω–µ—Ç —Ç—è–∂–µ–ª–æ –ø–æ –∫–≤–æ—Ç–µ.
        # –ü–æ—ç—Ç–æ–º—É –¥–∞—ë–º –≤—ã–±—Ä–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ UI:
        cat_titles = [f"{t} ({cid})" for cid, t in cats]
        selected = st.multiselect(
            "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –≤–æ–∑—å–º—ë–º –≤—Å–µ assignable)",
            options=cat_titles,
            default=cat_titles[: min(8, len(cat_titles))]  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ –≤—Å—ë, —á—Ç–æ–±—ã –Ω–µ —É–±–∏—Ç—å –∫–≤–æ—Ç—É
        )
        # —Ä–∞—Å–ø–∞—Ä—Å–∏–º –æ–±—Ä–∞—Ç–Ω–æ id
        selected_ids = []
        if selected:
            for s in selected:
                # "... (ID)"
                cid = s.split("(")[-1].split(")")[0].strip()
                selected_ids.append(cid)
        else:
            selected_ids = [cid for cid, _ in cats]

        for cid in selected_ids:
            video_ids.extend(most_popular_video_ids(region_code, pages, per_page, cid))
    else:
        video_ids.extend(most_popular_video_ids(region_code, pages, per_page, None))

    # dedupe
    video_ids = list(dict.fromkeys(video_ids))

    vids = fetch_videos(video_ids)
    subs_map = fetch_channels_subs(list({v["channelId"] for v in vids}))

    rows = []
    for v in vids:
        secs = iso_duration_to_seconds(v.get("duration_iso", ""))
        if exclude_shorts and secs < int(min_seconds):
            continue

        subs = subs_map.get(v["channelId"])
        days = age_days(v["publishedAt"]) if v.get("publishedAt") else None
        vpd = (v["views"] / days) if days else None

        ratio = None
        if subs and subs > 0:
            ratio = v["views"] / subs

        # —Ñ–∏–ª—å—Ç—Ä—ã
        if v["views"] < int(min_views):
            continue
        if subs is not None and subs > int(max_subs):
            continue
        if ratio is not None and ratio < float(min_ratio):
            continue

        rows.append({
            "title": v["title"],
            "channel": v["channelTitle"],
            "subs": subs,
            "views": v["views"],
            "ratio": round(ratio, 2) if ratio is not None else None,
            "views_per_day": round(vpd, 2) if vpd is not None else None,
            "publishedAt": v["publishedAt"],
            "duration_sec": secs,
            "thumbnail": v["thumbnail"],
            "url": f"https://www.youtube.com/watch?v={v['videoId']}",
        })

except HttpError as e:
    errors.append(str(e))

if errors:
    st.error("–û—à–∏–±–∫–∞ API (—á–∞—Å—Ç–æ –∫–≤–æ—Ç–∞/–∫–ª—é—á/–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞).")
    st.code("\n\n".join(errors))
    st.stop()

if not rows:
    st.info(
        "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥ —Ç–µ–∫—É—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã.\n\n"
        "–ü–æ–ø—Ä–æ–±—É–π:\n"
        "- —Å–Ω–∏–∑–∏—Ç—å min_views\n"
        "- –ø–æ–¥–Ω—è—Ç—å max_subs\n"
        "- –ø–æ—Å—Ç–∞–≤–∏—Ç—å min_ratio = 0‚Äì1 (—á–∞—Å—Ç–æ subs —Å–∫—Ä—ã—Ç—ã)\n"
        "- —É–≤–µ–ª–∏—á–∏—Ç—å pages/per_page\n"
        "- –≤–∫–ª—é—á–∏—Ç—å '–ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º' –∏ –≤—ã–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"
    )
    st.stop()

df = pd.DataFrame(rows)

# —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
if sort_mode == "views_per_day":
    df["_sort"] = df["views_per_day"].fillna(-1)
    df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])
elif sort_mode == "ratio":
    df["_sort"] = df["ratio"].fillna(-1)
    df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])
elif sort_mode == "views":
    df = df.sort_values("views", ascending=False)
else:
    df = df.sort_values("publishedAt", ascending=False)

st.success(f"–ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {len(df)}")

if view_mode.startswith("–ö–∞—Ä—Ç–æ—á–∫–∏"):
    for _, r in df.iterrows():
        c1, c2 = st.columns([1, 3])
        with c1:
            if r.get("thumbnail"):
                st.image(r["thumbnail"], use_container_width=True)
            else:
                st.write("üñº –ù–µ—Ç –ø—Ä–µ–≤—å—é")
        with c2:
            st.markdown(f"**{r['title']}**")
            st.write(
                f"–ö–∞–Ω–∞–ª: {r['channel']}\n\n"
                f"–ü–æ–¥–ø–∏—Å—á–∏–∫–∏: {fmt_int(r['subs'])} | –ü—Ä–æ—Å–º–æ—Ç—Ä—ã: {fmt_int(r['views'])} | "
                f"Views/day: {r.get('views_per_day','‚Äî')} | Ratio: {r.get('ratio','‚Äî')}\n\n"
                f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {fmt_int(r.get('duration_sec'))} —Å–µ–∫ | –î–∞—Ç–∞: {r['publishedAt']}"
            )
            st.markdown(f"[–û—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ]({r['url']})")
        st.divider()
else:
    show = df[["title", "channel", "subs", "views", "views_per_day", "ratio", "duration_sec", "publishedAt", "url"]]
    st.dataframe(show, use_container_width=True)

csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å CSV", data=csv_bytes, file_name=f"mostpopular_outliers_{region_code}.csv", mime="text/csv")

st.caption(
    "–í–∞–∂–Ω–æ: chart=mostPopular —á–∞—â–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫—Ä—É–ø–Ω—ã—Ö. –ù–æ —Ñ–∏–ª—å—Ç—Ä –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º –≤—ã—Ç–∞—â–∏—Ç —Ä–µ–¥–∫–∏–µ —Å–ª—É—á–∞–∏, "
    "–∫–æ–≥–¥–∞ –º–∞–ª–µ–Ω—å–∫–∏–π –∫–∞–Ω–∞–ª –ø–æ–ø–∞–ª –≤ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ."
)

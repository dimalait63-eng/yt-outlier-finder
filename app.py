# app.py â€” YouTube Outlier Finder (Ð±ÐµÐ· Shorts)
# Ð—Ð°Ð¿ÑƒÑÐº:
#   pip install streamlit google-api-python-client python-dotenv pandas
#   streamlit run app.py
#
# .env (Ð² Ð¿Ð°Ð¿ÐºÐµ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°):
#   YOUTUBE_API_KEY=Ð²Ð°Ñˆ_ÐºÐ»ÑŽÑ‡

import os
from datetime import datetime, timezone, date
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

load_dotenv()

API_KEY = os.getenv("YOUTUBE_API_KEY", "").strip()

st.set_page_config(page_title="YouTube Outlier Finder (Ð±ÐµÐ· Shorts)", layout="wide")
st.title("YouTube Outlier Finder (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾, Ð±ÐµÐ· Shorts)")

if not API_KEY:
    st.error("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½ YOUTUBE_API_KEY. Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ñ„Ð°Ð¹Ð» .env Ð¸ Ð´Ð¾Ð±Ð°Ð²ÑŒ ÐºÐ»ÑŽÑ‡.")
    st.stop()

youtube = build("youtube", "v3", developerKey=API_KEY)


def age_days(published_at: str) -> float:
    # "2026-02-25T12:34:56Z"
    dt = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
    days = (datetime.now(timezone.utc) - dt).total_seconds() / 86400
    return max(days, 0.1)


def fmt_int(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return "â€”"
    try:
        return f"{int(x):,}".replace(",", " ")
    except Exception:
        return "â€”"


@st.cache_data(ttl=1800)
def search_video_ids(query: str, max_results: int, published_after_iso: str | None,
                     order: str, duration_filter: str, region_code: str | None,
                     relevance_lang: str | None):
    """
    duration_filter: "long" | "medium" | "any"
    order: "viewCount" | "date" | "relevance"
    """
    params = dict(
        part="id",
        q=query,
        type="video",
        maxResults=max_results,
        order=order,
    )
    if published_after_iso:
        params["publishedAfter"] = published_after_iso

    # Ð’ÐÐ–ÐÐž: ÑÑ‚Ð¾ ÑƒÐ±Ð¸Ñ€Ð°ÐµÑ‚ Shorts Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð¿Ð¾Ð¸ÑÐºÐ°
    if duration_filter in ("long", "medium"):
        params["videoDuration"] = duration_filter  # long >= ~20 Ð¼Ð¸Ð½, medium 4â€“20 Ð¼Ð¸Ð½

    # Ð›Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ñ‹Ð´Ð°Ñ‡Ð¸ (Ð½Ðµ Ð²ÑÐµÐ³Ð´Ð° ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾, Ð½Ð¾ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚)
    if region_code:
        params["regionCode"] = region_code
    if relevance_lang:
        params["relevanceLanguage"] = relevance_lang

    r = youtube.search().list(**params).execute()
    return [it["id"]["videoId"] for it in r.get("items", [])]


@st.cache_data(ttl=1800)
def fetch_videos(video_ids: list[str]):
    out = []
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i + 50]
        r = youtube.videos().list(
            part="snippet,statistics,contentDetails",
            id=",".join(chunk)
        ).execute()

        for it in r.get("items", []):
            sn = it.get("snippet", {})
            stt = it.get("statistics", {})
            cd = it.get("contentDetails", {})

            thumbs = sn.get("thumbnails", {})
            thumb_url = None
            for k in ["maxres", "standard", "high", "medium", "default"]:
                if k in thumbs and "url" in thumbs[k]:
                    thumb_url = thumbs[k]["url"]
                    break

            out.append({
                "videoId": it.get("id"),
                "title": sn.get("title", ""),
                "channelId": sn.get("channelId", ""),
                "channelTitle": sn.get("channelTitle", ""),
                "publishedAt": sn.get("publishedAt", ""),
                "views": int(stt.get("viewCount", 0)),
                "duration_iso": cd.get("duration", ""),  # ISO 8601
                "thumbnail": thumb_url,
            })
    return out


@st.cache_data(ttl=1800)
def fetch_channels_subs(channel_ids: list[str]):
    result = {}
    channel_ids = [c for c in channel_ids if c]
    for i in range(0, len(channel_ids), 50):
        chunk = channel_ids[i:i + 50]
        r = youtube.channels().list(part="statistics", id=",".join(chunk)).execute()
        for it in r.get("items", []):
            stt = it.get("statistics", {})
            subs = stt.get("subscriberCount")
            result[it["id"]] = int(subs) if subs is not None else None
    return result


# ===== UI =====
with st.sidebar:
    st.header("ÐŸÐ¾Ð¸ÑÐº Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹")

    keywords_text = st.text_area(
        "ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° (ÐºÐ°Ð¶Ð´Ð¾Ðµ Ñ Ð½Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸)",
        value='psychology facts\n"dark facts"\ntrue crime documentary',
        height=140,
        help='ÐœÐ¾Ð¶Ð½Ð¾ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ñ„Ñ€Ð°Ð·Ñ‹ Ð² ÐºÐ°Ð²Ñ‹Ñ‡ÐºÐ°Ñ…, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ "dark facts".'
    )

    st.subheader("ÐšÐ°Ðº Ð¸ÑÐºÐ°Ñ‚ÑŒ")
    duration_filter = st.selectbox(
        "Ð”Ð»Ð¸Ð½Ð° Ð²Ð¸Ð´ÐµÐ¾ (ÑƒÐ±Ñ€Ð°Ñ‚ÑŒ Shorts)",
        options=[
            ("Ð¢Ð¾Ð»ÑŒÐºÐ¾ long (Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ 20+ Ð¼Ð¸Ð½ÑƒÑ‚)", "long"),
            ("Ð¢Ð¾Ð»ÑŒÐºÐ¾ medium (4â€“20 Ð¼Ð¸Ð½ÑƒÑ‚)", "medium"),
            ("Ð›ÑŽÐ±Ð°Ñ Ð´Ð»Ð¸Ð½Ð° (Ð¼Ð¾Ð¶ÐµÑ‚ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Shorts)", "any"),
        ],
        index=0,
        format_func=lambda x: x[0],
    )[1]

    order = st.selectbox(
        "Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð½Ð° ÑÑ‚Ð°Ð¿Ðµ Ð¿Ð¾Ð¸ÑÐºÐ° (Ð²Ð°Ð¶Ð½Ð¾!)",
        options=[
            ("ÐŸÐ¾ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð°Ð¼ (viewCount) â€” Ð»ÑƒÑ‡ÑˆÐµ Ð´Ð»Ñ Ð²Ð¸Ñ€ÑƒÑÐ½ÑÐºÐ°", "viewCount"),
            ("ÐŸÐ¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚Ð¸ (relevance)", "relevance"),
            ("ÐŸÐ¾ Ð½Ð¾Ð²Ð¸Ð·Ð½Ðµ (date)", "date"),
        ],
        index=0,
        format_func=lambda x: x[0],
    )[1]

    max_videos_per_kw = st.slider("Ð’Ð¸Ð´ÐµÐ¾ Ð½Ð° ÐºÐ»ÑŽÑ‡ (Ñ‡ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐµ, Ñ‚ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐµ)", 5, 50, 50, 5)

    days_back = st.slider("Ð˜ÑÐºÐ°Ñ‚ÑŒ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ N Ð´Ð½ÐµÐ¹", 1, 365, 90, 1)

    st.subheader("ÐŸÐ¾Ñ€Ð¾Ð³Ð¾Ð²Ñ‹Ðµ ÑƒÑÐ»Ð¾Ð²Ð¸Ñ")
    min_views = st.number_input("ÐœÐ¸Ð½. Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹", min_value=0, value=20000, step=5000)
    max_subs = st.number_input("ÐœÐ°ÐºÑ. Ð¿Ð¾Ð´Ð¿Ð¸ÑÑ‡Ð¸ÐºÐ¸ ÐºÐ°Ð½Ð°Ð»Ð°", min_value=0, value=200000, step=10000)
    min_ratio = st.number_input("ÐœÐ¸Ð½. Views/Subs (ÐµÑÐ»Ð¸ subs Ð²Ð¸Ð´Ð½Ñ‹)", min_value=0.0, value=3.0, step=1.0)

    st.subheader("Ð ÐµÐ³Ð¸Ð¾Ð½/ÑÐ·Ñ‹Ðº (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)")
    region_code = st.text_input("regionCode (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ RU, US, GB)", value="RU").strip().upper() or None
    relevance_lang = st.text_input("relevanceLanguage (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ ru, en)", value="ru").strip().lower() or None

    st.subheader("Ð’Ñ‹Ð²Ð¾Ð´")
    sort_mode = st.selectbox("Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²", ["ratio", "views_per_day", "views", "date"], index=1)
    view_mode = st.radio("Ð’Ð¸Ð´", ["ÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ (Ñ Ð¾Ð±Ð»Ð¾Ð¶ÐºÐ°Ð¼Ð¸)", "Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð°"], index=0)

    st.caption("Ð•ÑÐ»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¼Ð°Ð»Ð¾ â€” ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÑŒ days_back, max_videos_per_kw Ð¸ ÑÐ½Ð¸Ð·ÑŒ min_ratio/min_views.")

    run = st.button("ðŸ”Ž Ð¡ÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ")


if not run:
    st.write("Ð’Ð²ÐµÐ´Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° ÑÐ»ÐµÐ²Ð° â†’ Ð½Ð°Ð¶Ð¼Ð¸ **Ð¡ÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ**.")
    st.stop()

keywords = [k.strip() for k in keywords_text.splitlines() if k.strip()]
if not keywords:
    st.warning("Ð’Ð²ÐµÐ´Ð¸ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ð¾Ð´Ð½Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾.")
    st.stop()

published_after_iso = (datetime.now(timezone.utc) - pd.Timedelta(days=int(days_back))).isoformat().replace("+00:00", "Z")

# Ð°Ð½Ñ‚Ð¸-Ð¼ÑƒÑÐ¾Ñ€ (Ð¼ÑÐ³ÐºÐ¾)
BLOCK_WORDS = ["tiktok", "edit", "ÑÐ´Ð¸Ñ‚", "meme", "Ð¿Ñ€Ð¸ÐºÐ¾Ð»", "status", "reels"]
# ÐµÑÐ»Ð¸ Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ Ñ…Ð¾Ñ‡ÐµÑˆÑŒ Ð¸Ð½Ð¾Ð³Ð´Ð° Ð²Ð¸Ð´ÐµÑ‚ÑŒ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ â€” Ð²Ñ‹ÐºÐ»ÑŽÑ‡Ð¸ Ñ‡ÐµÑ€ÐµÐ· duration_filter="any"

all_rows = []
trend_rows = []
errors = []

try:
    for kw in keywords:
        ids = search_video_ids(
            kw, int(max_videos_per_kw), published_after_iso,
            order=order, duration_filter=duration_filter,
            region_code=region_code, relevance_lang=relevance_lang
        )
        ids = list(dict.fromkeys(ids))  # dedupe

        vids = fetch_videos(ids)
        subs_map = fetch_channels_subs(list({v["channelId"] for v in vids}))

        kw_rows = []
        for v in vids:
            title_l = (v.get("title") or "").lower()
            if any(w in title_l for w in BLOCK_WORDS):
                continue

            subs = subs_map.get(v["channelId"])
            days = age_days(v["publishedAt"]) if v.get("publishedAt") else None
            vpd = (v["views"] / days) if days else None

            ratio = None
            if subs and subs > 0:
                ratio = v["views"] / subs

            # Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹
            if v["views"] < int(min_views):
                continue
            if subs is not None and subs > int(max_subs):
                continue
            if ratio is not None and ratio < float(min_ratio):
                continue

            row = {
                "keyword": kw,
                "title": v["title"],
                "channel": v["channelTitle"],
                "subs": subs,
                "views": v["views"],
                "ratio": round(ratio, 2) if ratio is not None else None,
                "views_per_day": round(vpd, 2) if vpd is not None else None,
                "publishedAt": v["publishedAt"],
                "thumbnail": v["thumbnail"],
                "url": f"https://www.youtube.com/watch?v={v['videoId']}",
            }
            all_rows.append(row)
            kw_rows.append(row)

        if kw_rows:
            dfk = pd.DataFrame(kw_rows)
            trend_rows.append({
                "direction(keyword)": kw,
                "videos_found": int(len(dfk)),
                "avg_views_per_day": float(dfk["views_per_day"].fillna(0).mean()),
                "max_views_per_day": float(dfk["views_per_day"].fillna(0).max()),
                "outliers_count": int(dfk["ratio"].notna().sum()),
                "total_views": int(dfk["views"].sum()),
            })

except HttpError as e:
    errors.append(str(e))

if errors:
    st.error("ÐžÑˆÐ¸Ð±ÐºÐ° API. Ð§Ð°ÑÑ‚Ð¾ ÑÑ‚Ð¾ ÐºÐ²Ð¾Ñ‚Ð°/ÐºÐ»ÑŽÑ‡/Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°.")
    st.code("\n\n".join(errors))
    st.stop()

if not all_rows:
    st.info(
        "ÐÐ¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ð¾Ð´ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹.\n\n"
        "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹:\n"
        "- ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ 'Ð˜ÑÐºÐ°Ñ‚ÑŒ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ N Ð´Ð½ÐµÐ¹' Ð´Ð¾ 180â€“365\n"
        "- Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ min_ratio = 0â€“1 (Ñ‡Ð°ÑÑ‚Ð¾ subs ÑÐºÑ€Ñ‹Ñ‚Ñ‹)\n"
        "- Ð¿Ð¾Ð´Ð½ÑÑ‚ÑŒ max_subs Ð´Ð¾ 500k\n"
        "- order=viewCount (ÑƒÐ¶Ðµ ÑÑ‚Ð¾Ð¸Ñ‚) Ð¸ duration=long/medium\n"
        "- ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ 'Ð’Ð¸Ð´ÐµÐ¾ Ð½Ð° ÐºÐ»ÑŽÑ‡' Ð´Ð¾ 50"
    )
    st.stop()

df = pd.DataFrame(all_rows)

# ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
if sort_mode == "ratio":
    df["_sort"] = df["ratio"].fillna(-1)
    df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])
elif sort_mode == "views_per_day":
    df["_sort"] = df["views_per_day"].fillna(-1)
    df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])
elif sort_mode == "views":
    df = df.sort_values("views", ascending=False)
else:
    df = df.sort_values("publishedAt", ascending=False)

tab1, tab2 = st.tabs(["ðŸŽ¯ Ð’Ð¸Ð´ÐµÐ¾ (Ð±ÐµÐ· Shorts)", "ðŸ”¥ ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ"])

with tab2:
    if trend_rows:
        dft = pd.DataFrame(trend_rows).sort_values("avg_views_per_day", ascending=False)
        st.subheader("Ð¢Ð¾Ð¿ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ avg views/day (Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ñ‚Ð²Ð¾Ð¸Ñ… Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²)")
        st.dataframe(dft, use_container_width=True)
    else:
        st.info("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÑÐ¼ â€” ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¶Ñ‘ÑÑ‚ÐºÐ¸Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ð¸Ð»Ð¸ Ð¼Ð°Ð»Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ð¾ ÐºÐ»ÑŽÑ‡Ð°Ð¼.")

with tab1:
    st.subheader(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð²Ð¸Ð´ÐµÐ¾: {len(df)}")

    if view_mode.startswith("ÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸"):
        for _, r in df.iterrows():
            c1, c2 = st.columns([1, 3])
            with c1:
                if r.get("thumbnail"):
                    st.image(r["thumbnail"], use_container_width=True)
                else:
                    st.write("ðŸ–¼ ÐÐµÑ‚ Ð¿Ñ€ÐµÐ²ÑŒÑŽ")
            with c2:
                st.markdown(f"**{r['title']}**")
                st.write(f"ÐÐ°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ: {r['keyword']}")
                st.write(f"ÐšÐ°Ð½Ð°Ð»: {r['channel']}")
                st.write(
                    f"ÐŸÐ¾Ð´Ð¿Ð¸ÑÑ‡Ð¸ÐºÐ¸: {fmt_int(r['subs'])} | "
                    f"ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ñ‹: {fmt_int(r['views'])} | "
                    f"Views/day: {r.get('views_per_day','â€”')} | "
                    f"Ratio: {r.get('ratio','â€”')}"
                )
                st.write(f"Ð”Ð°Ñ‚Ð°: {r['publishedAt']}")
                st.markdown(f"[ÐžÑ‚ÐºÑ€Ñ‹Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾]({r['url']})")
            st.divider()
    else:
        show = df[["keyword", "title", "channel", "subs", "views", "views_per_day", "ratio", "publishedAt", "url"]]
        st.dataframe(show, use_container_width=True)

    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ CSV", data=csv_bytes, file_name="outliers.csv", mime="text/csv")

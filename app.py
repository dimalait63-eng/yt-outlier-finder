# app.py ‚Äî YouTube Most Popular Outliers (–±–µ–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –≤—ã–±–æ—Ä —Å—Ç—Ä–∞–Ω—ã, –±–µ–∑ Shorts)
#
# –ò–¥–µ—è:
# - –ë–µ—Ä—ë–º YouTube videos.list(chart="mostPopular") –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–µ
# - (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ä–∞—Å—à–∏—Ä—è–µ–º –æ—Ö–≤–∞—Ç, –ø—Ä–æ—Ö–æ–¥—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
# - –¢—è–Ω–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ + –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤ –∫–∞–Ω–∞–ª–∞
# - –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–≤–æ–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (views, subs, ratio, views/day)
# - –£–±–∏—Ä–∞–µ–º Shorts/–∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
#
# –ó–∞–ø—É—Å–∫:
#   pip install streamlit google-api-python-client python-dotenv pandas
#   streamlit run app.py
#
# .env:
#   YOUTUBE_API_KEY=–≤–∞—à_–∫–ª—é—á

import os
import re
from datetime import datetime, timezone

import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ---------- init ----------
load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY", "").strip()

st.set_page_config(page_title="YouTube Most Popular Outliers", layout="wide")
st.title("YouTube Most Popular Outliers (–±–µ–∑ –∫–ª—é—á–µ–π, –≤—ã–±–æ—Ä —Å—Ç—Ä–∞–Ω—ã, –±–µ–∑ Shorts)")

if not API_KEY:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω YOUTUBE_API_KEY. –°–æ–∑–¥–∞–π —Ñ–∞–π–ª .env –∏ –¥–æ–±–∞–≤—å –∫–ª—é—á.")
    st.stop()

youtube = build("youtube", "v3", developerKey=API_KEY)

# ---------- helpers ----------
DUR_RE = re.compile(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?")

def iso_duration_to_seconds(d: str) -> int:
    m = DUR_RE.fullmatch(d or "")
    if not m:
        return 0
    h = int(m.group(1) or 0)
    mi = int(m.group(2) or 0)
    s = int(m.group(3) or 0)
    return h * 3600 + mi * 60 + s

def age_days(published_at: str) -> float:
    dt = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
    days = (datetime.now(timezone.utc) - dt).total_seconds() / 86400
    return max(days, 0.1)

def fmt_int(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return "‚Äî"
    try:
        return f"{int(x):,}".replace(",", " ")
    except Exception:
        return "‚Äî"

def friendly_http_error(e: HttpError) -> str:
    try:
        content = e.content.decode("utf-8", errors="ignore")
    except Exception:
        content = str(e)
    return f"{e}\n\n{content}"

# ---------- API ----------
@st.cache_data(ttl=3600)
def fetch_categories(region_code: str):
    # –°–ø–∏—Å–æ–∫ assignable –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞
    r = youtube.videoCategories().list(part="snippet", regionCode=region_code).execute()
    cats = []
    for it in r.get("items", []):
        sn = it.get("snippet", {})
        if sn.get("assignable") is True:
            cats.append((it["id"], sn.get("title", it["id"])))
    cats.sort(key=lambda x: x[1].lower())
    return cats

@st.cache_data(ttl=1800)
def most_popular_video_ids(region_code: str, pages: int, per_page: int, category_id: str | None):
    # ‚úÖ –í–ê–ñ–ù–û: videos.list –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç part="id". –ù—É–∂–Ω–æ part="snippet" (–∏–ª–∏ –¥—Ä—É–≥–æ–µ –≤–∞–ª–∏–¥–Ω–æ–µ).
    ids = []
    page_token = None
    for _ in range(pages):
        params = dict(
            part="snippet",
            chart="mostPopular",
            regionCode=region_code,
            maxResults=per_page,
        )
        if category_id:
            params["videoCategoryId"] = category_id
        if page_token:
            params["pageToken"] = page_token

        r = youtube.videos().list(**params).execute()
        ids.extend([it["id"] for it in r.get("items", [])])

        page_token = r.get("nextPageToken")
        if not page_token:
            break

    return list(dict.fromkeys(ids))

@st.cache_data(ttl=1800)
def fetch_videos(video_ids: list[str]):
    out = []
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        r = youtube.videos().list(
            part="snippet,statistics,contentDetails",
            id=",".join(chunk)
        ).execute()

        for it in r.get("items", []):
            sn = it.get("snippet", {})
            stt = it.get("statistics", {})
            cd = it.get("contentDetails", {})

            thumbs = sn.get("thumbnails", {})
            thumb_url = None
            for k in ["maxres", "standard", "high", "medium", "default"]:
                if k in thumbs and "url" in thumbs[k]:
                    thumb_url = thumbs[k]["url"]
                    break

            out.append({
                "videoId": it.get("id"),
                "title": sn.get("title", ""),
                "channelId": sn.get("channelId", ""),
                "channelTitle": sn.get("channelTitle", ""),
                "publishedAt": sn.get("publishedAt", ""),
                "views": int(stt.get("viewCount", 0)),
                "likes": int(stt.get("likeCount", 0)) if "likeCount" in stt else None,
                "comments": int(stt.get("commentCount", 0)) if "commentCount" in stt else None,
                "duration_iso": cd.get("duration", ""),
                "thumbnail": thumb_url,
            })
    return out

@st.cache_data(ttl=1800)
def fetch_channels_subs(channel_ids: list[str]):
    result = {}
    channel_ids = [c for c in channel_ids if c]
    for i in range(0, len(channel_ids), 50):
        chunk = channel_ids[i:i+50]
        r = youtube.channels().list(part="statistics", id=",".join(chunk)).execute()
        for it in r.get("items", []):
            stt = it.get("statistics", {})
            subs = stt.get("subscriberCount")
            result[it["id"]] = int(subs) if subs is not None else None
    return result

# ---------- UI ----------
COMMON_COUNTRIES = [
    ("US", "United States"),
    ("GB", "United Kingdom"),
    ("CA", "Canada"),
    ("AU", "Australia"),
    ("DE", "Germany"),
    ("FR", "France"),
    ("NL", "Netherlands"),
    ("ES", "Spain"),
    ("IT", "Italy"),
    ("BR", "Brazil"),
    ("MX", "Mexico"),
    ("JP", "Japan"),
    ("KR", "South Korea"),
    ("IN", "India"),
    ("RU", "Russia"),
    ("UA", "Ukraine"),
    ("PL", "Poland"),
    ("SE", "Sweden"),
    ("NO", "Norway"),
    ("TR", "Turkey"),
]

with st.sidebar:
    st.header("–ò—Å—Ç–æ—á–Ω–∏–∫")
    country_choice = st.selectbox(
        "–°—Ç—Ä–∞–Ω–∞ (regionCode)",
        options=COMMON_COUNTRIES,
        index=0,
        format_func=lambda x: f"{x[1]} ({x[0]})",
    )
    region_code = country_choice[0]

    st.caption("–ë–µ—Ä—ë–º –¥–∞–Ω–Ω—ã–µ –∏–∑ YouTube chart=mostPopular –ø–æ —Å—Ç—Ä–∞–Ω–µ (—ç—Ç–æ –Ω–µ –≤–µ—Å—å YouTube, –Ω–æ —Ö–æ—Ä–æ—à–∏–π '–ø–æ–ø—É–ª—è—Ä–Ω—ã–π' —Å—Ä–µ–∑).")

    st.subheader("–û–±—ä—ë–º –≤—ã–±–æ—Ä–∫–∏")
    pages = st.slider("–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±—Ä–∞—Ç—å", 1, 10, 3, 1)
    per_page = st.selectbox("–í–∏–¥–µ–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É", [10, 25, 50], index=2)
    use_categories = st.checkbox("–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—à–∏—Ä–µ –æ—Ö–≤–∞—Ç)", value=True)

    st.subheader("–§–∏–ª—å—Ç—Ä—ã")
    exclude_shorts = st.checkbox("–ò—Å–∫–ª—é—á–∏—Ç—å Shorts/–∫–æ—Ä–æ—Ç–∫–∏–µ", value=True)
    min_seconds = st.slider("–ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (—Å–µ–∫)", 60, 1800, 120, 30, disabled=not exclude_shorts)

    min_views = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", min_value=0, value=50_000, step=10_000)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–∏ –∫–∞–Ω–∞–ª–∞", min_value=0, value=200_000, step=10_000)
    min_ratio = st.number_input("–ú–∏–Ω. Views/Subs (–µ—Å–ª–∏ subs –≤–∏–¥–Ω—ã)", min_value=0.0, value=3.0, step=1.0)

    st.subheader("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞/–≤–∏–¥")
    sort_mode = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ", ["views_per_day", "ratio", "views", "date"], index=0)
    view_mode = st.radio("–í–∏–¥", ["–ö–∞—Ä—Ç–æ—á–∫–∏ (—Å –æ–±–ª–æ–∂–∫–∞–º–∏)", "–¢–∞–±–ª–∏—Ü–∞"], index=0)

    run = st.button("üîé –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å")

if not run:
    st.write("–í—ã–±–µ—Ä–∏ —Å—Ç—Ä–∞–Ω—É —Å–ª–µ–≤–∞ ‚Üí –Ω–∞–∂–º–∏ **–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å**.")
    st.stop()

# ---------- run ----------
errors: list[str] = []

try:
    video_ids: list[str] = []

    if use_categories:
        cats = fetch_categories(region_code)
        cat_options = [f"{title} ({cid})" for cid, title in cats]

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º –í–°–ï, —á—Ç–æ–±—ã –Ω–µ —É–±–∏—Ç—å –∫–≤–æ—Ç—É: –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ä—É–∫–∞–º–∏
        default_pick = cat_options[: min(8, len(cat_options))]

        selected = st.multiselect(
            "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –≤–æ–∑—å–º—ë–º –≤—Å–µ assignable)",
            options=cat_options,
            default=default_pick,
        )

        selected_ids: list[str] = []
        if selected:
            for s in selected:
                cid = s.split("(")[-1].split(")")[0].strip()
                selected_ids.append(cid)
        else:
            selected_ids = [cid for cid, _ in cats]

        for cid in selected_ids:
            video_ids.extend(most_popular_video_ids(region_code, int(pages), int(per_page), cid))
    else:
        video_ids.extend(most_popular_video_ids(region_code, int(pages), int(per_page), None))

    video_ids = list(dict.fromkeys(video_ids))  # dedupe

    vids = fetch_videos(video_ids)
    subs_map = fetch_channels_subs(list({v["channelId"] for v in vids}))

    rows = []
    for v in vids:
        secs = iso_duration_to_seconds(v.get("duration_iso", ""))

        if exclude_shorts and secs < int(min_seconds):
            continue

        subs = subs_map.get(v["channelId"])
        days = age_days(v["publishedAt"]) if v.get("publishedAt") else None
        vpd = (v["views"] / days) if days else None

        ratio = None
        if subs and subs > 0:
            ratio = v["views"] / subs

        if v["views"] < int(min_views):
            continue
        if subs is not None and subs > int(max_subs):
            continue
        if ratio is not None and ratio < float(min_ratio):
            continue

        rows.append({
            "title": v["title"],
            "channel": v["channelTitle"],
            "subs": subs,
            "views": v["views"],
            "views_per_day": round(vpd, 2) if vpd is not None else None,
            "ratio": round(ratio, 2) if ratio is not None else None,
            "duration_sec": secs,
            "publishedAt": v["publishedAt"],
            "thumbnail": v["thumbnail"],
            "url": f"https://www.youtube.com/watch?v={v['videoId']}",
        })

except HttpError as e:
    errors.append(friendly_http_error(e))

if errors:
    st.error("–û—à–∏–±–∫–∞ API (–∫–≤–æ—Ç–∞/–∫–ª—é—á/–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏).")
    st.code("\n\n".join(errors))
    st.stop()

if not rows:
    st.info(
        "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥ —Ç–µ–∫—É—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã.\n\n"
        "–ü–æ–ø—Ä–æ–±—É–π:\n"
        "- —Å–Ω–∏–∑–∏—Ç—å min_views\n"
        "- –ø–æ–¥–Ω—è—Ç—å max_subs\n"
        "- –ø–æ—Å—Ç–∞–≤–∏—Ç—å min_ratio = 0‚Äì1 (—á–∞—Å—Ç–æ subs —Å–∫—Ä—ã—Ç—ã)\n"
        "- —É–≤–µ–ª–∏—á–∏—Ç—å pages/per_page\n"
        "- –≤–∫–ª—é—á–∏—Ç—å '–ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º' –∏ –≤—ã–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n"
        "- —É–º–µ–Ω—å—à–∏—Ç—å '–ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å' (–µ—Å–ª–∏ —Å—Ä–µ–∑ —Å–ª–∏—à–∫–æ–º —É–∑–∫–∏–π)"
    )
    st.stop()

df = pd.DataFrame(rows)

# sort
if sort_mode == "views_per_day":
    df["_sort"] = df["views_per_day"].fillna(-1)
    df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])
elif sort_mode == "ratio":
    df["_sort"] = df["ratio"].fillna(-1)
    df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])
elif sort_mode == "views":
    df = df.sort_values("views", ascending=False)
else:
    df = df.sort_values("publishedAt", ascending=False)

st.success(f"–ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {len(df)}")

if view_mode.startswith("–ö–∞—Ä—Ç–æ—á–∫–∏"):
    for _, r in df.iterrows():
        c1, c2 = st.columns([1, 3])
        with c1:
            if r.get("thumbnail"):
                st.image(r["thumbnail"], use_container_width=True)
            else:
                st.write("üñº –ù–µ—Ç –ø—Ä–µ–≤—å—é")
        with c2:
            st.markdown(f"**{r['title']}**")
            st.write(
                f"–ö–∞–Ω–∞–ª: {r['channel']}\n\n"
                f"–ü–æ–¥–ø–∏—Å—á–∏–∫–∏: {fmt_int(r['subs'])} | "
                f"–ü—Ä–æ—Å–º–æ—Ç—Ä—ã: {fmt_int(r['views'])} | "
                f"Views/day: {r.get('views_per_day','‚Äî')} | "
                f"Ratio: {r.get('ratio','‚Äî')}\n\n"
                f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {fmt_int(r.get('duration_sec'))} —Å–µ–∫ | "
                f"–î–∞—Ç–∞: {r['publishedAt']}"
            )
            st.markdown(f"[–û—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ]({r['url']})")
        st.divider()
else:
    show = df[["title", "channel", "subs", "views", "views_per_day", "ratio", "duration_sec", "publishedAt", "url"]]
    st.dataframe(show, use_container_width=True)

csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å CSV",
    data=csv_bytes,
    file_name=f"mostpopular_outliers_{region_code}.csv",
    mime="text/csv",
)

st.caption(
    "–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: chart=mostPopular —á–∞—Å—Ç–æ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –∫—Ä—É–ø–Ω—ã–µ –∫–∞–Ω–∞–ª—ã. "
    "–§–∏–ª—å—Ç—Ä –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º –≤—ã—Ç–∞—â–∏—Ç —Ä–µ–¥–∫–∏–µ —Å–ª—É—á–∞–∏, –≥–¥–µ –º–∞–ª–µ–Ω—å–∫–∏–π –∫–∞–Ω–∞–ª –ø–æ–ø–∞–ª –≤ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ."
)
